{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e89601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_similarity\n",
    "import networkx as nx\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed80f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasapy in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from farasapy) (2.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from farasapy) (4.66.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->farasapy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5FD909E40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/farasapy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5FD90A170>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/farasapy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5FD90A320>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/farasapy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5FD90A4D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/farasapy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5FD90A680>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/farasapy/\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip  install -U farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8884a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad El Hakawati\\AppData\\Local\\Temp\\ipykernel_4048\\806539116.py:1: DtypeWarning: Columns (15,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\Mohammad El Hakawati\\Downloads\\Complaints dataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Mohammad El Hakawati\\Downloads\\Complaints dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22b64b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['CASE_DESC'].str.len() >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d14b7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasa in c:\\users\\mohammad el hakawati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install farasa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02eb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_path=r\"C:\\Users\\Mohammad El Hakawati\\Downloads\\technical-support.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f319fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51c9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance():\n",
    "    selected_option = search_option.get()\n",
    "\n",
    "    # Assuming lowest_euclidean, lowest_manhattan, and highest_cosine are defined somewhere in your code\n",
    "\n",
    "    new_window = tk.Toplevel()  # Create a new window\n",
    "\n",
    "    if selected_option == 1:  # Euclidean Distance (basic string similarity)\n",
    "        result_label = tk.Label(new_window, text=\"lowest_euclidean[['CASE_DESC', 'Euclidean Distances']]\")\n",
    "        result_label.pack()\n",
    "    elif selected_option == 2:  # Manhattan Distance (word count difference)\n",
    "        result_label = tk.Label(new_window, text=\"lowest_manhattan[['CASE_DESC', 'Manhattan Distances']]\")\n",
    "        result_label.pack()\n",
    "    elif selected_option == 3:  # Cosine Similarity (basic overlap of keywords)\n",
    "        result_label = tk.Label(new_window, text=\"highest_cosine[['CASE_DESC', 'Cosine Similarity']]\")\n",
    "        result_label.pack()\n",
    "    else:\n",
    "        result_label = tk.Label(new_window, text=\"Invalid measure selected.\")\n",
    "        result_label.pack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e3eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Student Info System\")\n",
    "i_path = r\"C:\\Users\\Mohammad El Hakawati\\Downloads\\technical-support.png\"\n",
    "img = tk.PhotoImage(file=i_path)\n",
    "img = img.subsample(1, 1)\n",
    "\n",
    "colored_box = tk.Canvas(root, width=root.winfo_screenwidth(), height=30, bg='#81b0d4')\n",
    "colored_box.pack(side=\"top\", fill=tk.X)\n",
    "\n",
    "# Create a label inside the canvas with a larger font size\n",
    "label_id = tk.Label(colored_box, text=\"Enter Student ID\", font=('Helvetica', 25), fg='#023047', bg='#81b0d4')\n",
    "label_id.pack(pady=5)\n",
    "\n",
    "label_img = tk.Label(root, image=img)\n",
    "label_img.pack(side=\"right\", padx=10, pady=10)\n",
    "\n",
    "label_id = tk.Label(root, text=\"Enter your query below\", font=('Helvetica', 20), fg='#023047', bg='#81b0d4')\n",
    "label_id.pack(side=\"top\", pady=50)\n",
    "\n",
    "entry_id = tk.Entry(root)\n",
    "entry_id.pack(side=\"top\", ipadx=170, ipady=10)\n",
    "\n",
    "search_option = tk.IntVar()\n",
    "\n",
    "# Create a frame to hold the radio buttons\n",
    "radio_frame = tk.Frame(root)\n",
    "\n",
    "radio_euclidean = tk.Radiobutton(radio_frame, text=\"Euclidean Distance\", font=('Helvetica', 17), fg='#023047', variable=search_option, value=1)\n",
    "radio_manhattan = tk.Radiobutton(radio_frame, text=\"Manhattan Distance\", font=('Helvetica', 17), fg='#023047', variable=search_option, value=2)\n",
    "radio_cosine = tk.Radiobutton(radio_frame, text=\"Cosine Similarity\", font=('Helvetica', 17), fg='#023047', variable=search_option, value=3)\n",
    "\n",
    "radio_euclidean.pack(side=\"top\", padx=400, pady=5, ipadx=50)\n",
    "radio_manhattan.pack(side=\"top\", padx=400, pady=5, ipadx=50)\n",
    "radio_cosine.pack(side=\"top\", padx=400, pady=5, ipadx=50)\n",
    "\n",
    "# Create the button with the command set to calculate_distance\n",
    "b = tk.Button(radio_frame, text=\"Search\", command=calculate_distance, font=('Helvetica', 17), fg='#023047', bg='#81b0d4')\n",
    "b.pack(side=\"bottom\", pady=10)\n",
    "\n",
    "# Pack the radio_frame into the root window\n",
    "radio_frame.pack(side=\"left\", padx=20, pady=20)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3912192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    def tokenize_arabic_text(text):\n",
    "        # Tokenization: Detect and isolate individual words\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "\n",
    "    def lemmatize_single_word(word):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        return lemmatized_word\n",
    "\n",
    "    def word_representation(word):\n",
    "        # Word representation: Introduce \"EXTRA-SEKOUN\" diacritical mark for missed diacritics\n",
    "        # Assume the input word is a sequence of Arabic letters and diacritical marks\n",
    "        new_word = ''\n",
    "        for char in word:\n",
    "            if char.isalpha() or char in ['.', 'EXTRA-SEKOUN']:\n",
    "                new_word += char\n",
    "        return new_word\n",
    "\n",
    "    def remove_definite_article(word):\n",
    "        # Regular expression pattern to match Arabic words\n",
    "        arabic_word_pattern = re.compile(r'\\b\\w+\\b', re.U)\n",
    "\n",
    "        # Check if the word is in Arabic and contains the definite article 'ال'\n",
    "        if re.match(arabic_word_pattern, word) and word.startswith('ال'):\n",
    "            # Remove the definite article\n",
    "            word_without_article = word[len('ال',):].replace('و', '')\n",
    "            return word_without_article\n",
    "        else:\n",
    "            # Return the original word if it doesn't start with 'ال' or if it's not an Arabic word\n",
    "            return word\n",
    "\n",
    "    def reduce_orthographic_ambiguity(word):\n",
    "        # Define mapping for orthographic ambiguity reduction\n",
    "        orthographic_mapping = {\n",
    "            'ة': 'ه',   # Taa marbuta to Ha\n",
    "            'ؤ': 'ء',   # Waw hamza to Hamza\n",
    "            'إ': 'ا',   # Alef hamza below to Alef\n",
    "            'أ': 'ا',   # Alef hamza above to Alef\n",
    "            'ء': 'ا',   # Hamza to Alef\n",
    "            'ئ': 'ء',   # Yeh hamza to Hamza\n",
    "            'ٱ': 'ا',   # Alef wasla to Alef\n",
    "            'آ': 'ا'    # Alef with madda to Alef\n",
    "        }\n",
    "\n",
    "        # Replace characters based on the defined mapping\n",
    "        for original, replacement in orthographic_mapping.items():\n",
    "            word = word.replace(original, replacement)\n",
    "\n",
    "        return word\n",
    "\n",
    "    def keep_specific_numeric_patterns(text):\n",
    "        # Your implementation to keep specific numeric patterns\n",
    "        # For example, you can use regular expressions to remove certain numeric patterns\n",
    "        # Replace the following line with your implementation\n",
    "        processed_text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "        return processed_text\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # 3. Remove punctuation and short words, excluding consecutive dots\n",
    "    tokens = [token for token in tokens if re.match(r'\\w', token) and len(token) > 1 and not re.match(r'\\.{2,}', token)]\n",
    "\n",
    "    # Remove punctuation for both English and Arabic text\n",
    "    english_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    arabic_tokens = [token for token in tokens if re.search('[\\u0600-\\u06FF]', token)]\n",
    "\n",
    "    # Preprocess Arabic and English tokens\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'^(079|078|077|00962|962|\\+962)\\d+$', token):\n",
    "            processed_tokens.append(token)\n",
    "        elif re.search('[\\u0600-\\u06FF]', token):  # Check if the token contains Arabic characters\n",
    "            # Apply Arabic preprocessing steps\n",
    "            arabic_tokens = tokenize_arabic_text(token)\n",
    "            arabic_tokens = [reduce_orthographic_ambiguity(word_representation(remove_definite_article(word))) for word in arabic_tokens]\n",
    "            processed_tokens.extend(arabic_tokens)\n",
    "        else:\n",
    "            # Apply English preprocessing steps\n",
    "            token = lemmatize_single_word(token.lower())\n",
    "            processed_tokens.append(token)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words_arabic = set(stopwords.words('arabic'))\n",
    "    most_popular_stopwords_arabic = {\"هي\", \"هي\",\"على\", \"من\", \"لم\", \"تم\", \".\", \"،\", \"أنا\", \"أنت\", \"إلى\", \"في\", \"عن\", \"هذا\", \"هذه\", \"هو\", \"وهي\", \"ثم\", \"..\"}\n",
    "    combined_stops_arabic = stop_words_arabic.union(most_popular_stopwords_arabic)\n",
    "    processed_tokens = [token for token in processed_tokens if token not in combined_stops_arabic]\n",
    "\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    processed_tokens = [token for token in processed_tokens if token not in stop_words_english]\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_tokens = [stemmer.stem(token) for token in processed_tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    # Apply the specific numeric pattern removal logic\n",
    "    processed_text = keep_specific_numeric_patterns(processed_text)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Assuming 'df' is a DataFrame with a column named 'CASE_DESC'\n",
    "# Apply the preprocess_text function to the 'CASE_DESC' column\n",
    "df['CASE_DESC'] = df['CASE_DESC'].apply(preprocess_text)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df['CASE_DESC'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
